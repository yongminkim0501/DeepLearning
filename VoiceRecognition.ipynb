{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "16WY6dFI29ygsUWpLqk-oxYivARRNAEoq",
      "authorship_tag": "ABX9TyOGhTMXl28hzMO9zg6W8U/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongminkim0501/DeepLearning/blob/main/VoiceRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ifgv4aQPEhkz"
      },
      "outputs": [],
      "source": [
        "# 기본 데이터 분석 및 과학 계산 라이브러리\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import os\n",
        "# 음성 처리 관련 라이브러리\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "\n",
        "# 딥러닝 라이브러리\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from transformers import pipeline, AutoModelForAudioClassification, AutoFeatureExtractor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "음성인식 라이브러리"
      ],
      "metadata": {
        "id": "aM1PurWDeMUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV7FJkEVdlYM",
        "outputId": "7dac201d-355d-4e8d-a25f-210707e16c33"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "구글드라이브 연결"
      ],
      "metadata": {
        "id": "NbgJPUm3eFhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/VoiceRecognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz0BXSSGeN9J",
        "outputId": "981b963d-bd14-4ea1-bd99-17834d804729"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VoiceRecognition\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tar_file = '/content/drive/MyDrive/VoiceRecognition/download.tar'\n",
        "extract_dir = '/content/drive/MyDrive/VoiceRecognition'\n",
        "\n",
        "!mkdir -p {extract_dir}\n",
        "!tar -xf {tar_file} -C {extract_dir}"
      ],
      "metadata": {
        "id": "AI1huUYgehKq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/VoiceRecognition/KoreanVoice"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3tHEP20iDtr",
        "outputId": "a03e846f-b1fb-4273-85cc-682e26eadc23"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VoiceRecognition/KoreanVoice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 조각들을 하나로 합치기\n",
        "!cat KsponSpeech_01.zip.part* > KsponSpeech_01.zip\n",
        "\n",
        "# zip 파일 압축 해제\n",
        "!unzip KsponSpeech_01.zip -d /content/drive/MyDrive/VoiceRecognition/UnpackedKoreanVoice"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04772c2-90cc-4aa9-ad75-0f49f33c3a24",
        "id": "qlwOF7xjiEGx"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  KsponSpeech_01.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of KsponSpeech_01.zip or\n",
            "        KsponSpeech_01.zip.zip, and cannot find KsponSpeech_01.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzK2oqpFkgh1",
        "outputId": "90be47ef-28c9-417f-ab21-758d5a1780dd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw------- 1 root root 15301122689 Mar 25 04:29 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip\n",
            "-rw------- 1 root root  1073741824 Mar 24 09:12 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part0\n",
            "-rw------- 1 root root  1073741824 Mar 24 09:14 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part1073741824\n",
            "-rw------- 1 root root  1073741824 Mar 24 09:30 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part10737418240\n",
            "-rw------- 1 root root  1073741824 Mar 24 09:31 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part11811160064\n",
            "-rw------- 1 root root  1073741824 Mar 24 09:32 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part12884901888\n",
            "-rw------- 1 root root  1073741824 Mar 24 09:33 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part13958643712\n",
            "-rw------- 1 root root   268737153 Mar 24 09:35 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part15032385536\n",
            "-rw------- 1 root root  1073741824 Mar 24 09:15 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part2147483648\n",
            "-rw------- 1 root root  1073741824 Mar 24 09:19 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part3221225472\n",
            "-rw------- 1 root root  1073741824 Mar 24 09:20 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part4294967296\n",
            "-rw------- 1 root root  1073741824 Mar 24 09:22 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part5368709120\n",
            "-rw------- 1 root root  1073741824 Mar 24 09:24 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part6442450944\n",
            "-rw------- 1 root root  1073741824 Mar 24 09:25 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part7516192768\n",
            "-rw------- 1 root root  1073741824 Mar 24 09:27 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part8589934592\n",
            "-rw------- 1 root root  1073741824 Mar 24 09:28 /content/drive/MyDrive/VoiceRecognition/KoreanVoice/KsponSpeech_01.zip.part9663676416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat KsponSpeech_01.zip.part* > KsponSpeech_complete.zip"
      ],
      "metadata": {
        "id": "7C_DkMT3kovP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la KsponSpeech_complete.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5raqjEVmim2",
        "outputId": "642a2e2b-0fce-4c23-afbb-89c6bd1dce54"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw------- 1 root root 15301122689 Mar 25 04:37 KsponSpeech_complete.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p KsponSpeech_extracted"
      ],
      "metadata": {
        "id": "Wh96-amdmtFh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la KsponSpeech_extracted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqCKKbCwmydL",
        "outputId": "e42650bb-fde8-4e80-803c-15af625f0738"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip KsponSpeech_complete.zip -d KsponSpeech_extracted"
      ],
      "metadata": {
        "id": "IbvH6beDnHDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!file KsponSpeech_01.zip.part0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yJmLhnInR_K",
        "outputId": "22188bce-5152-4c78-ec62-adca31ba3a13"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KsponSpeech_01.zip.part0: Zip archive data, at least v2.0 to extract, compression method=store\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install p7zip-full -y && 7z x KsponSpeech_01.zip.part0 -oKsponSpeech_extracted && ls -la KsponSpeech_extracted"
      ],
      "metadata": {
        "id": "L6iFMXnjnaYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------ 데이터 처리를 위한 기본 과정 (실패) 다시 해야함\n",
        "------------------"
      ],
      "metadata": {
        "id": "3kYaRyuYs8qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플링 레이트 설정\n",
        "fs = 22050  # 일반적인 오디오 샘플링 레이트\n",
        "\n",
        "# 임시 음성 데이터 생성 (약 1초 길이의 신호)\n",
        "duration = 1.0  # 1초\n",
        "t = np.linspace(0, duration, int(fs * duration), endpoint=False)\n",
        "\n",
        "# 여러 주파수 성분을 가진 신호 생성\n",
        "# 기본 주파수 (예: 말소리의 기본 주파수 범위에 해당)\n",
        "f0 = 120  # Hz (남성 목소리의 기본 주파수에 가까움)\n",
        "# 하모닉스 추가\n",
        "data = 0.5 * np.sin(2 * np.pi * f0 * t)  # 기본 주파수\n",
        "data += 0.3 * np.sin(2 * np.pi * 2 * f0 * t)  # 2배 하모닉\n",
        "data += 0.15 * np.sin(2 * np.pi * 3 * f0 * t)  # 3배 하모닉\n",
        "data += 0.05 * np.sin(2 * np.pi * 4 * f0 * t)  # 4배 하모닉\n",
        "\n",
        "# 약간의 노이즈 추가\n",
        "noise = np.random.normal(0, 0.05, len(data))\n",
        "data += noise"
      ],
      "metadata": {
        "id": "tBPms6GBt7KX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data = 0                 # 원래는 특정 데이터가 들어가야함\n",
        "n_fft = 2048\n",
        "hop = n_fft // 4\n",
        "stft = librosa.stft(data, n_fft=n_fft, window='hann', hop_length=hop, center=True)\n",
        "print('stft.shape', stft.shape)\n",
        "# STFT 계산(출력차원: (n_fft//2) + 1 , T ) =(1025,13)\n",
        "# T = (L - n_fft) // hop_length + 1(=구간개수=13)\n",
        "\n",
        "# Mel Filter Bank 계수 계산\n",
        "mel_f = librosa.filters.mel(sr=fs, n_fft=n_fft, n_mels=128)\n",
        "print('mel_f.shape',mel_f.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFNcX_zkd4rq",
        "outputId": "d7b94bf1-4166-415b-82f5-cb033535654d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stft.shape (1025, 44)\n",
            "mel_f.shape (128, 1025)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MFCC (Mel Frequency Cepstral Coefficients)의 약자\n",
        "음성 신호의 특징을 추출하는 기술\n",
        "1. STFT(Short Time Fourier Transform)에 의해 주어진 음성 신호를 작은 프레임 단위로 나누어서 주파수 영역의 데이터로 변환\n",
        "2. Mel Filter Bank로 멜 스펙트럼을 계산\n",
        "3. 로그 스케일링하고 DCT(Discrete Cosine Transform)을 수행\n",
        "4. 이를 이용하여 해당 프레임의 특징을 추출\n",
        "-> 이렇게 추출된 특징은 일반적으로 MFCC 계수라고 부름\n",
        "\n",
        "출처 : https://wikidocs.net/193361"
      ],
      "metadata": {
        "id": "IwpvK1V7gI1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/VoiceRecognition\")\n",
        "file_path = \"/content/drive/MyDrive/VoiceRecognition\"\n",
        "train_path = file_path + \"/train\"\n",
        "test_path = file_path + \"/test\""
      ],
      "metadata": {
        "id": "xx1UG3kyiU60"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "기본적인 데이터를 불러오기 위한 주소 정리"
      ],
      "metadata": {
        "id": "0xH8XMdMkA4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y,sr = librosa.load(file_path, sr=None) #sr=None은 원본 샘플링 레이트 유지\n",
        "\n",
        "voice_normalized = librosa.util.normalize(y)\n",
        "target_sr = 0                           # 리샘플링 관련 샘플링 관련하여 필요 시, 데이터 전처리하며 분석하여 진행\n",
        "y_resampled = librosa.resample(y, orig_sr = sr, target_sr = target_sr)"
      ],
      "metadata": {
        "id": "gvEx5Su2iFEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "리샘플링 관련 코드"
      ],
      "metadata": {
        "id": "FGv7EnqykBVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.signal as signal\n",
        "b, a = signal.butter(5, 100/(sr/2), 'highpass')\n",
        "y_filtered = signal.filtfilt(b, a, y)   # highpass 필터를 이용한 간단한 노이즈 필터링\n",
        "                                        # 또는 스펙트럴 서브 트랙션 등 더 복잡한 노이즈 제거 방법 사용"
      ],
      "metadata": {
        "id": "y4_rOTdEjn0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "노이즈 필터링 관련된 부분"
      ],
      "metadata": {
        "id": "IZcQO0R5kAKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# 모델 및 토크나이저 로드\n",
        "tokenizer = BertTokenizer.from_pretrained(\"snunlp/KR-BERT-char16424\")\n",
        "model = TFBertForSequenceClassification.from_pretrained(\n",
        "    \"snunlp/KR-BERT-char16424\",\n",
        "    num_labels=3,\n",
        "    from_pt=True  # PyTorch 가중치를 TensorFlow로 변환\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GMtp_8Sj-83",
        "outputId": "67a00933-1f37-4fb5-8ab8-74c1f72388b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import os\n",
        "from transformers import WhisperProcessor, TFWhisperForConditionalGeneration\n",
        "from transformers import WhisperTokenizer\n",
        "from datasets import Dataset, Audio\n",
        "import evaluate\n",
        "\n",
        "class WhisperFineTuner:\n",
        "    def __init__(self, model_name=\"openai/whisper-small\", batch_size=16):\n",
        "        self.model_name = model_name\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # 모델 및 프로세서 초기화\n",
        "        print(f\"모델 로딩 중: {model_name}\")\n",
        "        self.processor = WhisperProcessor.from_pretrained(model_name)\n",
        "        self.tokenizer = WhisperTokenizer.from_pretrained(model_name, language=\"ko\", task=\"transcribe\")\n",
        "\n",
        "        # 모델 로드 (from_pt=True로 PyTorch 가중치 변환)\n",
        "        self.model = TFWhisperForConditionalGeneration.from_pretrained(\n",
        "            model_name, from_pt=True\n",
        "        )\n",
        "\n",
        "        # WER(Word Error Rate) 메트릭 초기화\n",
        "        self.metric = evaluate.load(\"wer\")\n",
        "\n",
        "    def prepare_dataset(self, data_path, csv_file=None):\n",
        "        \"\"\"\n",
        "        데이터셋 준비\n",
        "        - data_path: 오디오 파일이 있는 디렉토리 경로\n",
        "        - csv_file: 오디오 파일 경로와 텍스트 전사 내용이 있는 CSV 파일\n",
        "                   (columns: ['file_path', 'transcription'])\n",
        "        \"\"\"\n",
        "        if csv_file:\n",
        "            # CSV에서 데이터 로드\n",
        "            df = pd.read_csv(csv_file)\n",
        "            data_dict = {\n",
        "                \"file_path\": df[\"file_path\"].tolist(),\n",
        "                \"transcription\": df[\"transcription\"].tolist()\n",
        "            }\n",
        "        else:\n",
        "            # CSV가 없을 경우 디렉토리에서 오디오 파일만 수집\n",
        "            file_paths = [os.path.join(data_path, f) for f in os.listdir(data_path)\n",
        "                         if f.endswith(('.wav', '.mp3', '.flac'))]\n",
        "\n",
        "            # 이 경우 전사 데이터가 없으므로 빈 문자열로 초기화 (준지도 학습에 활용 가능)\n",
        "            data_dict = {\n",
        "                \"file_path\": file_paths,\n",
        "                \"transcription\": [\"\"] * len(file_paths)\n",
        "            }\n",
        "\n",
        "        # Hugging Face 데이터셋 생성\n",
        "        dataset = Dataset.from_dict(data_dict)\n",
        "\n",
        "        # 오디오 로드 설정\n",
        "        dataset = dataset.cast_column(\"file_path\", Audio(sampling_rate=16000))\n",
        "\n",
        "        # 전처리 함수 정의\n",
        "        def prepare_dataset(examples):\n",
        "            # 오디오 파일 로드\n",
        "            audio = examples[\"file_path\"]\n",
        "\n",
        "            # 샘플링 레이트 확인 및 변환\n",
        "            if isinstance(audio, dict) and \"array\" in audio:\n",
        "                speech = audio[\"array\"]\n",
        "                sampling_rate = audio[\"sampling_rate\"]\n",
        "            else:\n",
        "                # librosa로 직접 로드\n",
        "                speech, sampling_rate = librosa.load(examples[\"file_path\"], sr=16000)\n",
        "\n",
        "            # Whisper 모델용 입력 준비\n",
        "            input_features = self.processor.feature_extractor(\n",
        "                speech, sampling_rate=sampling_rate, return_tensors=\"tf\"\n",
        "            ).input_features[0]\n",
        "\n",
        "            # 레이블(전사 텍스트) 토큰화\n",
        "            label = examples[\"transcription\"]\n",
        "            labels = self.tokenizer(label, return_tensors=\"tf\").input_ids[0]\n",
        "\n",
        "            return {\n",
        "                \"input_features\": input_features,\n",
        "                \"labels\": labels\n",
        "            }\n",
        "\n",
        "        # 데이터셋에 전처리 함수 적용\n",
        "        processed_dataset = dataset.map(prepare_dataset)\n",
        "\n",
        "        # 학습 및 검증 데이터 분할\n",
        "        train_test_split = processed_dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "        return train_test_split\n",
        "\n",
        "    def compute_metrics(self, pred):\n",
        "        \"\"\"평가 메트릭 계산\"\"\"\n",
        "        pred_ids = np.argmax(pred.predictions, axis=-1)\n",
        "\n",
        "        # 레이블과 예측을 디코딩\n",
        "        pred_str = self.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "        label_ids = np.where(pred.label_ids != -100, pred.label_ids, self.tokenizer.pad_token_id)\n",
        "        label_str = self.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "        # WER 계산\n",
        "        wer = self.metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "        return {\"wer\": wer}\n",
        "\n",
        "    def data_collator(self, features):\n",
        "        \"\"\"데이터 배치 처리\"\"\"\n",
        "        input_features = [feature[\"input_features\"] for feature in features]\n",
        "        labels = [feature[\"labels\"] for feature in features]\n",
        "\n",
        "        # 패딩 처리\n",
        "        batch = {\n",
        "            \"input_features\": tf.stack(input_features),\n",
        "            \"labels\": tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                labels, padding=\"post\", value=self.tokenizer.pad_token_id\n",
        "            )\n",
        "        }\n",
        "\n",
        "        return batch\n",
        "\n",
        "    def finetune(self, dataset_dict, output_dir=\"./whisper-finetuned-korean\", epochs=3):\n",
        "        \"\"\"모델 파인튜닝\"\"\"\n",
        "        # 옵티마이저 및 학습 설정\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "\n",
        "        # 모델 컴파일\n",
        "        self.model.compile(optimizer=optimizer)\n",
        "\n",
        "        # 데이터셋을 tf.data.Dataset으로 변환\n",
        "        def to_tf_dataset(dataset):\n",
        "            def gen():\n",
        "                for item in dataset:\n",
        "                    yield {\n",
        "                        \"input_features\": item[\"input_features\"],\n",
        "                        \"labels\": item[\"labels\"]\n",
        "                    }\n",
        "\n",
        "            return tf.data.Dataset.from_generator(\n",
        "                gen,\n",
        "                output_signature={\n",
        "                    \"input_features\": tf.TensorSpec(shape=(80, None), dtype=tf.float32),\n",
        "                    \"labels\": tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
        "                }\n",
        "            ).padded_batch(self.batch_size)\n",
        "\n",
        "        train_dataset = to_tf_dataset(dataset_dict[\"train\"])\n",
        "        eval_dataset = to_tf_dataset(dataset_dict[\"test\"])\n",
        "\n",
        "        # 모델 학습\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                filepath=os.path.join(output_dir, \"checkpoint-{epoch}\"),\n",
        "                save_weights_only=True\n",
        "            ),\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor=\"val_loss\", patience=2, restore_best_weights=True\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # 모델 학습\n",
        "        history = self.model.fit(\n",
        "            train_dataset,\n",
        "            validation_data=eval_dataset,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        # 모델 저장\n",
        "        self.model.save_pretrained(output_dir)\n",
        "        self.processor.save_pretrained(output_dir)\n",
        "\n",
        "        return history\n",
        "\n",
        "    def evaluate(self, test_dataset):\n",
        "        \"\"\"모델 평가\"\"\"\n",
        "        predictions = []\n",
        "        references = []\n",
        "\n",
        "        for batch in test_dataset:\n",
        "            # 추론\n",
        "            pred = self.model.generate(\n",
        "                input_features=batch[\"input_features\"],\n",
        "                language=\"ko\",\n",
        "                task=\"transcribe\"\n",
        "            )\n",
        "\n",
        "            # 디코딩\n",
        "            decoded_preds = self.tokenizer.batch_decode(pred, skip_special_tokens=True)\n",
        "            decoded_labels = self.tokenizer.batch_decode(\n",
        "                batch[\"labels\"],\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "            predictions.extend(decoded_preds)\n",
        "            references.extend(decoded_labels)\n",
        "\n",
        "        # WER 계산\n",
        "        wer = self.metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "        return {\"wer\": wer}\n",
        "\n",
        "\n",
        "# 사용 예시\n",
        "if __name__ == \"__main__\":\n",
        "    # 파인튜너 초기화\n",
        "    finetuner = WhisperFineTuner(model_name=\"openai/whisper-small\")\n",
        "\n",
        "    # 데이터셋 준비\n",
        "    # 참고: 실제 파일 경로와 CSV 파일을 사용해야 함\n",
        "    dataset = finetuner.prepare_dataset(\n",
        "        data_path=\"./korean_audio_data\",\n",
        "        csv_file=\"./transcriptions.csv\"\n",
        "    )\n",
        "\n",
        "    # 모델 파인튜닝\n",
        "    history = finetuner.finetune(dataset, epochs=5)\n",
        "\n",
        "    # 모델 평가\n",
        "    metrics = finetuner.evaluate(dataset[\"test\"])\n",
        "    print(f\"Word Error Rate: {metrics['wer']}\")"
      ],
      "metadata": {
        "id": "CmScRN0tnuuL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}